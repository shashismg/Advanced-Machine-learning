# ğŸš€ Advanced Machine Learning Models â€“ AdaBoost | XGBoost | CatBoost

Hey there! ğŸ‘‹  
This repo contains my exploration of some **advanced ensemble learning models** that are widely used in modern ML competitions and real-world applications:

- âš¡ AdaBoost
- ğŸš€ XGBoost
- ğŸˆ CatBoost

These models are all about **boosting** â€” improving weak learners into strong ones. This repo helped me understand the intuition behind them, how to use them in Python, and how to tune them for better performance.

---

## ğŸ“ Files Overview

| File | Model Explored |
|------|----------------|
| `adaboost_basics.ipynb` | Intro to AdaBoost using Decision Trees as base estimators |
| `xgboost_basics.ipynb` | XGBoost implementation + feature importance visualization |
| `catboost_basics.ipynb` | CatBoost for handling categorical features effectively |

Each notebook includes:
- Model intuition and how boosting works  
- Code implementation using real datasets  
- Evaluation with accuracy, confusion matrix, ROC curves, etc.  
- A bit of hyperparameter tuning where possible  

---

## ğŸ› ï¸ Tools & Libraries Used

- Python 3.x
- Scikit-learn
- XGBoost
- CatBoost
- Pandas, NumPy
- Matplotlib, Seaborn

---

## ğŸ¯ Why This Repo?

This is just part of my learning log â€” not production code or a fancy tutorial.  
I built these to:
- Get familiar with the boosting technique  
- Learn the differences between AdaBoost, XGBoost, and CatBoost  
- Practice model training, evaluation, and comparison

---


> _â€œBoost your knowledge, one weak learner at a time.â€_
